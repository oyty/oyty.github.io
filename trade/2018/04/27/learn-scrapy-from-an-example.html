<html>
<head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0 user-scalable=yes"/>
    <link href="https://oyty.me/main.css" rel="stylesheet" type="text/css">
    <link href="https://oyty.me/codehighlight.css" rel="stylesheet" type="text/css">
    <link rel="alternate" type="application/atom+xml" href="/atom.xml" title="Atom feed">
    <link rel="shortcut icon" href="https://oyty.me/images/logo.ico">
    <title><code>[Scrapy]</code>从一个简单例子了解Scrapy</title>
</head>
<body>
  <div class="content">
    <h2><code>[Scrapy]</code>从一个简单例子了解Scrapy</h2>

<h4>创建Scrapy项目</h4>

<p>进入到你要创建项目的目录，键入下面命令：
<code>
oyty-mbp:python oyty$ scrapy startproject tutorial
New Scrapy project &#39;tutorial&#39;, using template directory &#39;/usr/local/lib/python3.6/site-packages/scrapy/templates/project&#39;, created in:
    /Users/oyty/Documents/newworkspace/python/tutorial
</code>
创建了一个<code>tutorial</code>的目录，内容如下：
```
tutorial/
    scrapy.cfg            # deploy configuration file</p>

<pre><code>tutorial/             # project&#39;s Python module, you&#39;ll import your code from here
    __init__.py

    items.py          # project items definition file

    middlewares.py    # project middlewares file

    pipelines.py      # project pipelines file

    settings.py       # project settings file

    spiders/          # a directory where you&#39;ll later put your spiders
        __init__.py
</code></pre>

<p>```</p>

<h4>第一个爬虫</h4>

<p>Scrapy使用Spiders的类去爬取一个或多个网页，这些类必须是<code>scrapy.Spider</code>的子类，一定要定义初始的请求，然后你可以选择性地解析网页里面的links，下载和解析网页内容。
创建我们第一个爬虫类：<code>tutorial/spiders/quotes_spider.py</code>
```
import scrapy</p>

<p>class QuotesSpider(scrapy.Spider):
    name = &quot;quotes&quot;</p>

<pre><code>def start_requests(self):
    urls = [
        &#39;http://quotes.toscrape.com/page/1/&#39;,
        &#39;http://quotes.toscrape.com/page/2/&#39;,
    ]

    for url in urls:
        yield scrapy.Request(url=url, callback=self.parse)

def parse(self, response):
    page = response.url.split(&quot;/&quot;)[-2]
    print(page)
    filename = &#39;quotes-%s.html&#39; % page
    with open(filename, &#39;wb&#39;) as f:
        f.write(response.body)
    self.log(&#39;Saved file %s&#39; % filename)
</code></pre>

<p>``<code>
-</code>name<code>：作为Spider的标识，它必须是唯一的，也就是说你在同一个项目中你不能为不同的爬虫类设置相同的name。
-</code>start_requests()<code>：必须返回可迭代的</code>Requests<code>对象（你可以返回一个requests的数组，也可以返回一个生成器），Scrapy将从这些请求开始爬取，后续的请求在这些初始请求后产生。
-</code>parse()<code>：解析方法，每一个请求后都会被调用，参数</code>response<code>是一个</code>TextResponse`对象，这个对象包含了网页的内容，并且有内置了很多有用的处理网页内容的方法。</p>

<p><code>parse()</code>方法通常用来解析response，将网页内容解析为字典，然后在网页中找到新的URLs，再创建新的爬取请求。</p>

<h4>运行我们的爬虫</h4>

<p>在项目的跟目录下，运行下面命令：
<code>
scrapy crawl quotes
</code>
第二个参数就是我们定义在爬虫类中的<code>name</code>。</p>

<p>这个地方要明白一点的是，我们<code>start_requests</code>中有多个请求，这些请求是异步发起，后续的response解析也都是相互独立的，这样大大提升了爬虫的效率。</p>

<h4><code>start_requests</code>的一种快捷方式</h4>

<p>在<code>scrapy.Spider</code>类中已经定义了<code>start_requests</code>方法：
```
def <strong>init</strong>(self, name=None, **kwargs):
    if name is not None:
        self.name = name
    elif not getattr(self, &#39;name&#39;, None):
        raise ValueError(&quot;%s must have a name&quot; % type(self).<strong>name</strong>)
    self.<strong>dict</strong>.update(kwargs)
    if not hasattr(self, &#39;start<em>urls&#39;):
        self.start</em>urls = []</p>

<p>def start<em>requests(self):
    cls = self.</em><em>class</em>_
    if method<em>is</em>overridden(cls, Spider, &#39;make<em>requests</em>from<em>url&#39;):
        warnings.warn(
            &quot;Spider.make</em>requests<em>from</em>url method is deprecated; it &quot;
            &quot;won&#39;t be called in future Scrapy releases. Please &quot;
            &quot;override Spider.start<em>requests method instead (see %s.%s).&quot; % (
                cls.</em><em>module</em><em>, cls.</em><em>name</em>_
            ),
        )
        for url in self.start<em>urls:
            yield self.make</em>requests<em>from</em>url(url)
    else:
        for url in self.start<em>urls:
            yield Request(url, dont</em>filter=True)
<code>
从上面的代码我们可以知道，首先呢，`make_requests_from_url`这个方法已经过期了，所以啊你最好不要用了，要用`start_requests`方法。`start_requests`方法默认其实是返回了`start_urls`的迭代，而`start_urls`是Spider类的一个属性，子类自然也就获取了这个属性，所以只要在子类中初始化`start_urls`就可以了，不用再重写   `start_requests`方法。
</code>
import scrapy</p>

<p>class QuotesSpider(scrapy.Spider):
    name = &quot;quotes&quot;
    start_urls = [
        &#39;http://quotes.toscrape.com/page/1/&#39;,
        &#39;http://quotes.toscrape.com/page/2/&#39;,
    ]</p>

<pre><code>def parse(self, response):
    page = response.url.split(&quot;/&quot;)[-2]
    filename = &#39;quotes-%s.html&#39; % page
    with open(filename, &#39;wb&#39;) as f:
        f.write(response.body)
</code></pre>

<p>```</p>

  </div>


</body>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-115892756-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-115892756-1');
</script>


</html>